{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to take data using the scope and plot relevant histograms. Will output energy resolution and associated errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyvisa as visa\n",
    "\n",
    "#sys.path.insert(0, '..\\scope-daq')\n",
    "\n",
    "import MSO4102Bastro as sdaq # This is the scope module that Sean G. wrote. You will need this module (should be on GitHub, https://github.com/ibrewer/scope-daq)\n",
    "import importlib\n",
    "#importlib.reload(sdaq)\n",
    "import time\n",
    "import h5py # This is the python library that creates files/stores data sets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter the IP address of the scope as a string. The IP address of the scope should be set by the router (make sure both the scope and the lab laptop are plugged into the router). To check the IP address of the scope, you can go to the Utility menu and check the LAN settings. Sometimes a LAN reset is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = sdaq.Scope(address=\"169.254.2.185\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define output file and data set names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='../dataOut/101921_amp1/'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "outName='cobalt57_14h'\n",
    "dsName='run1'\n",
    "scope.set_source_channel(0) #Set scope channel to read - same as displayed on scope\n",
    "traces=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boolean run options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePlots=True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This command creates a h5py file, desginated as \"f.\" Documentation for h5py can be found at http://docs.h5py.org/en/stable/. \n",
    "### NOTE: Please keep the 'a' flag.  ***Also, make sure you close an open file (use f.close()) before you open a new one.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = h5py.File(path+outName+'_scaling.h5py', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an array that stores the scope scaling dictionary. This will make sure that we have the scope settings for any given run.\n",
    "\n",
    "### If you want to extract the scope scaling parameters from a file, the 5 settings are stored in the order [x zero, x incr, y zero, y mult, y offset].\n",
    "### To extract the data, you could say ***data_scale = f['scope_scaling']*** and then extract the values that you want using ***data_scale[1] = x_increment***, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scaling_dict = scope.read_scaling_config()\n",
    "\n",
    "scaling_info = np.zeros(5)\n",
    "scaling_info[0] = float(scaling_dict['XZERO'])\n",
    "scaling_info[1] = float(scaling_dict['XINCR'])\n",
    "scaling_info[2] = float(scaling_dict['YZERO'])\n",
    "scaling_info[3] = float(scaling_dict['YMULT'])\n",
    "scaling_info[4] = float(scaling_dict['YOFF'])\n",
    "\n",
    "#AMANDA - debug error with same dataset name from hardcoded \"scope_scaling\"\n",
    "dset = g.create_dataset(dsName+'_scaling', data=scaling_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The get_data function will initialize a run (a full run length is 30000 traces, which takes approx. an hour and a half)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The naming scheme for data sets is filename_run#, ex. \"BC_2019703_0822_45_plateau1_initial_run1.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This function pulls SiPM pulses from the scope and stores them in arrays.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    no_of_traces: float\n",
    "        However many traces or SiPM pulses you want to record.\n",
    "    data_set_name: string\n",
    "        Name of the data set within the file. The naming scheme for data sets is \n",
    "        filename_run#, ex. \"BurstCube_PostVibe_Cs137_061419_run1.\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    Data sets. Two data sets should be created, the scope scaling information and \n",
    "    The scope data is stored in an array (a h5py data set). h5py data sets are nice \n",
    "    because you can splice into them. In this case, the size of the array is determined by the \n",
    "    number of traces you want from the scope and number of data points the scope collects for \n",
    "    each trace; the size of the array is number of traces by amount of scope points. Each row \n",
    "    is a scope trace, so plotting/data analysis is done in a for loop that looks at each row \n",
    "    one at a time.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "def get_data(run_time, data_set_name, no_of_traces = 100, noise_range = (0, 2000), signal_range = (2000,10000)):\n",
    "\n",
    "    # Determines number of points from each scope trace and creates an empty array\n",
    "    _, last_trace = scope.read_triggered_event()\n",
    "    last_trace = np.array([int(s) for s in last_trace.split(',')])\n",
    "    \n",
    "    curve_length = len(last_trace)\n",
    "    #arr = np.zeros((no_of_traces, curve_length))\n",
    "    arr = [] \n",
    "    \n",
    "    # Scaling dictionary is used to scale the scope traces to account for scope settings\n",
    "    scaling_dict = scope.read_scaling_config()\n",
    "    \n",
    "    peaks = []\n",
    "    integrals = []\n",
    "    \n",
    "    n_bad_comm = 0\n",
    "    i = 0\n",
    "    t_start = time.time()\n",
    "    t_end = t_start + run_time * 60 #run time in minutes\n",
    "    n_dup = 0\n",
    "    single = True\n",
    "    \n",
    "    time_axis = None\n",
    "    \n",
    "    bla = time.strftime('%a, %d %b %Y %H:%M:%S', time.localtime(t_end) )\n",
    "    \n",
    "    print(f\"Starting {run_time} minute run; will be done at {bla}\")\n",
    "    \n",
    "    while time.time() < t_end:\n",
    "        # Code to discount duplicates (when the scope gets stuck on a trigger):\n",
    "        \n",
    "        try: \n",
    "            _, trace = scope.read_triggered_event()\n",
    "            trace = np.array([int(s) for s in trace.split(',')])\n",
    "            if np.sum(trace - last_trace) == 0:\n",
    "                i -= 1\n",
    "                n_dup +=1\n",
    "                print(\"%d duplicates\" %(n_dup))\n",
    "            else:\n",
    "                last_trace = trace\n",
    "                time_scaled, trace_scaled = scope.scale_data(scaling_dict, trace)\n",
    "                \n",
    "                if (no_of_traces < 0) or (i < no_of_traces):\n",
    "                    arr.append(trace_scaled)\n",
    "                    time_axis = time_scaled\n",
    "                #if i % 1000 == 0:\n",
    "                #    print(\"At {0:d} / {1:d}\".format(i, no_of_traces))\n",
    "                \n",
    "                noise_sample = np.mean(trace_scaled[noise_range[0]:noise_range[1]])\n",
    "                trace_scaled -= noise_sample\n",
    "                peaks.append( np.max(trace_scaled)  )\n",
    "                integrals.append( np.sum(trace_scaled[signal_range[0]:signal_range[1]] ) )    \n",
    "                \n",
    "        \n",
    "        # Code to override Visa errors:\n",
    "        except visa.VisaIOError:\n",
    "            n_bad_comm += 1\n",
    "            print(\"Communication timeout... %d\" %(n_bad_comm))\n",
    "            i -= 1 \n",
    "        \n",
    "        i += 1       \n",
    "    \n",
    "        #if i > 0 and i %1000 == 0 and single:\n",
    "        #    t_now = time.time()\n",
    "        #    single = False \n",
    "        #    elapsed = time.time() - t_start\n",
    "        #    rate = float(i)/float(t_now - t_start)\n",
    "        #    print(\"{2:s} ; At {0:d}/{1:d}\".format(i, no_of_traces, time.strftime('%a, %d %b %Y %H:%M:%S GMT', time.localtime())))\n",
    "        #    print(\"\\tRate: {0:6.3f} Hz\\t Elapsed: {1:6.2f} s\\t Estimated total run length: {2:6.2f} s\\t Estimated time remaining: {3:6.2f}\".format(rate, elapsed, no_of_traces/(rate), no_of_traces/(rate) - elapsed))\n",
    "\n",
    "        #if i%1000 == 1:\n",
    "        #    single = True\n",
    "    \n",
    "    t_stop = time.time()\n",
    "    \n",
    "    run_len = t_stop - t_start\n",
    "    run_min = run_len / 60\n",
    "    print(f\"Recorded {i} traces in {run_min:0.3f} minutes. Average rate: {i/run_len:.2f} Hz\")\n",
    "\n",
    "    \n",
    "    dset = f.create_dataset(data_set_name, data=np.array(arr))    \n",
    "    dset = f.create_dataset(data_set_name+\"_t\", data=np.array(time_axis))   \n",
    "\n",
    "    dset = f.create_dataset(data_set_name+\"_peaks\", data=np.array(peaks))   \n",
    "    dset = f.create_dataset(data_set_name+\"_integral\", data=np.array(integrals))   \n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually take data, double check the data set name. Remember the first function input is the number of traces and the second input is the data set name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(path+outName+'.h5py', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 840 minute run; will be done at Wed, 20 Oct 2021 06:30:29\n",
      "1 duplicates\n"
     ]
    }
   ],
   "source": [
    "get_data(14*60, dsName, traces) # take data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the data sets within a file (check to make sure your run is there):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis/plotting: assign the data set as the array \"plot_array.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_array = f[dsName] # insert desired data set name here\n",
    "time_axis = np.array(f[dsName+\"_t\"])\n",
    "#sanity check\n",
    "if len(plot_array)!=traces: \n",
    "    print(\"ERROR - SOMETHING AWRY \\n dataset length is not what was input - are you looking at the right dataset?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 10% of all traces to look at the data and perform a common sense check. Also look for a stretch of data that can be used to determine noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(int(traces*0.1)):\n",
    "    plt.plot(time_axis*1e6, plot_array[i])\n",
    "    \n",
    "plt.xlabel('time [us]')\n",
    "plt.ylabel('peak amplitude [V]')\n",
    "\n",
    "if savePlots:\n",
    "    plt.savefig(path+outName+'_'+dsName+'_traces.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function will extract the peak from each trace/row of the data set. The noise sample is determined by eye; pick a range of x values where there don't appear to be (many) peaks, ex. from 2000:3500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#peaks = np.zeros(len(plot_array))\n",
    "#integ = np.zeros(len(plot_array))\n",
    "#max_point = 0\n",
    "#noise_sample = 0\n",
    "\n",
    "#for i in range(len(plot_array)):\n",
    "#    max_point = np.max(plot_array[i])\n",
    "#    noise_sample = np.mean(plot_array[i,0:2000])\n",
    "#    peaks[i] = max_point - noise_sample\n",
    "    #integral = integral of trace after trigger - integral of noise (before trigger)\n",
    "    #trigger at trace point 2000\n",
    "#    integ[i] = np.sum(plot_array[i,2000:10000]) - np.sum(plot_array[i,0:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your peak values. Save this data set name as ***data set name_peaks*** ex. \"BurstCube_PostVibe_Cs137_061419_run1_peaks.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peaks = f[dsName + '_peaks' ]\n",
    "integ = f[dsName + \"_integral\"]\n",
    "\n",
    "#print (np.allclose(read_peaks, peaks, rtol=0.01, atol=0.01))\n",
    "\n",
    "#print(read_peaks[0:10]-peaks[0:10])\n",
    "#print(read_integral[0:10]-integ[0:10] )\n",
    "\n",
    "#print(np.allclose(read_integral, integ))\n",
    "\n",
    "#dset = f.create_dataset(dsName+'_peaks', data=peaks)\n",
    "#dset = f.create_dataset(dsName+'_integral', data=integ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a histogram of the peaks using matplotlib to get a sense of what the data looks like and where the peak is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ultra fine binning for debugging input\n",
    "plt.hist(peaks, bins=180)\n",
    "plt.xlabel('peak amplitude [V]')\n",
    "plt.ylabel('counts')\n",
    "\n",
    "if savePlots:\n",
    "    plt.savefig(path+outName+'_'+dsName+'_peakHist.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a more rigorous histogram and fit the data to find the energy resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AMANDA - optimize x axis range and bin number (so not hardcoded to 1 and 1000)\n",
    "bins = np.linspace(0,1,1000) # Here, the number of bins has been set to 100 and the length of\n",
    "# the x axis has been set to one. This can be adjusted; inspect the matplotlib histogram to\n",
    "# see what the binning / x axis should be.\n",
    "fit_hist, bins_1 = np.histogram(peaks, bins=bins)\n",
    "bins_2 = np.array([bins[i] for i in range(len(bins)-1)])\n",
    "#bins_2 = 0.5*(bins[0:-1]+bins[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The scipy.optimize function will fit the histogram of your peaks to a Gaussian. The x data is the bins_2 array and the y data is the output from the np.histogram function. In order for the fit to work, you have to make reasonable guesses for the amplitude, the mean, and the sigma of the histogram.\n",
    "### Change the title of the plot, save figure if it looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.optimize import curve_fit\n",
    "import pylab \n",
    "\n",
    "x = bins_2\n",
    "y = fit_hist\n",
    "xspace = np.linspace(0, 1, 10000) # This creates a smoother plot when plotting the fit\n",
    "\n",
    "#AMANDA - automate initial parameter setting\n",
    "# Guesses for p01: [Amplitude, Mu, Sigma]. These guesses must be reasonable.\n",
    "mean = sum(peaks)/len(peaks)\n",
    "p01 = [traces, mean, 0.05]\n",
    "\n",
    "# Define the fit function (a Gaussian)\n",
    "def Gauss(x, A, mu, sigma):\n",
    "    return A*np.exp(-(x-mu)**2/(2.0*sigma**2))\n",
    "\n",
    "#AMANDA - include check that fit converges\n",
    "# popt returns the best fit values for amplitude, mean, and sigma. pcov returns a covariance\n",
    "# matrix; the diagonal of this matrix returns the errors associated with the three returned \n",
    "# values, which is used to determine the error in the energy resolution.\n",
    "popt, pcov = curve_fit(Gauss, xdata=bins_2, ydata=fit_hist, p0=p01, maxfev=5000)\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.xlim([0,mean*2.5])\n",
    "\n",
    "# Plot the data\n",
    "plt.bar(bins_2, fit_hist, width=bins[1] - bins[0], color='blue', label=r'Data')\n",
    "# Plot the fit\n",
    "plt.plot(xspace, Gauss(xspace, *popt), 'r-', label='Fit')\n",
    "(Amp, Mu, Sigma) = popt\n",
    "# Print the outputs \n",
    "print(popt)\n",
    "print(\"Amplitude = %d, Mu = %0.4f, Sigma = %0.4f\" %(Amp, Mu, Sigma))\n",
    "energy_res = (2.355*Sigma*100)/Mu # Calculates energy resolution, 2.355 converts sigma to FWHM\n",
    "print(\"The energy resolution is approximately %0.2f percent.\" %(abs(energy_res)))\n",
    "plt.legend()\n",
    "plt.title('Energy Resolution Post Vibe')\n",
    "plt.xlabel('Energy (V)')\n",
    "plt.ylabel('Counts')\n",
    "\n",
    "\n",
    "if savePlots:\n",
    "    energy_res_str = str(abs(round(energy_res,2)))\n",
    "    plt.savefig(path+outName+'_'+dsName+'_'+energy_res_str+'enRes.png')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the errors associated with the energy resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Amp_err, Mu_err, Sigma_err) = np.sqrt(np.diag(pcov))\n",
    "# Error propagation\n",
    "partial_sigma = (2.355*100)/Mu\n",
    "partial_mu = (2.355*100*Sigma)/(Mu**2)\n",
    "stdev_er = np.sqrt(((partial_sigma**2)*(Sigma_err**2))+((partial_mu**2)*(Mu_err)**2))\n",
    "print(\"Error in amplitude is %0.3f. \\nError in mu is %0.6f. \\nError in sigma is %0.6f.\" %(Amp_err, Mu_err, Sigma_err))\n",
    "print(\"The error in the energy resolution is %0.5f percent.\"%(stdev_er))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the same for the trace integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ultra fine binning for debugging input\n",
    "plt.hist(integ, bins=180)\n",
    "plt.xlabel('trace integral [V]')\n",
    "plt.ylabel('counts')\n",
    "\n",
    "if savePlots:\n",
    "    plt.savefig(path+outName+'_'+dsName+'_integHist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit integral distribution and pull out mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_integ = np.linspace(0,1000,100) # Here, the number of bins has been set to 100 and the length of\n",
    "# the x axis has been set to one. This can be adjusted; inspect the matplotlib histogram to\n",
    "# see what the binning / x axis should be.\n",
    "fit_hist2, bins_integ_1 = np.histogram(integ, bins=bins_integ)\n",
    "bins_integ_2 = np.array([bins_integ[i] for i in range(len(bins_integ)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bins_integ_2\n",
    "y = fit_hist2\n",
    "xspace = np.linspace(0, 1000, 10000) # This creates a smoother plot when plotting the fit\n",
    "\n",
    "#AMANDA - automate initial parameter setting\n",
    "# Guesses for p01: [Amplitude, Mu, Sigma]. These guesses must be reasonable.\n",
    "print(mean)\n",
    "print(traces)\n",
    "mean = sum(integ)/len(integ)\n",
    "p01 = [traces, mean, traces/10.]\n",
    "\n",
    "#fit the data\n",
    "popt, pcov = curve_fit(Gauss, xdata=bins_integ_2, ydata=fit_hist2, p0=p01,maxfev=5000)\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "# Plot the data\n",
    "plt.bar(bins_integ_2, fit_hist2, width=bins_integ[1] - bins_integ[0], color='blue', label=r'Data')\n",
    "# Plot the fit\n",
    "plt.plot(xspace, Gauss(xspace, *popt), 'r-', label='Fit')\n",
    "(Amp_integ, Mu_integ, Sigma_integ) = popt\n",
    "# Print the outputs \n",
    "print(popt)\n",
    "print(\"Amplitude = %d, Mu = %0.4f, Sigma = %0.4f\" %(Amp_integ, Mu_integ, Sigma_integ))\n",
    "\n",
    "plt.xlim([0,mean*2.5])\n",
    "energy_res_integ = (2.355*Sigma*100)/Mu # Calculates energy resolution, 2.355 converts sigma to FWHM\n",
    "print(\"The energy resolution is approximately %0.2f percent.\" %(abs(energy_res_integ)))\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Integrated Energy of pulse (V)')\n",
    "plt.ylabel('Counts')\n",
    "\n",
    "if savePlots:\n",
    "    energy_res_str = str(abs(round(energy_res_integ,2)))\n",
    "    plt.savefig(path+outName+'_'+dsName+'_'+energy_res_str+'enRes_integ.png')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save energy resolution values in txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=open(path+outName+\"_enRes.txt\", \"a\")\n",
    "k.write(dsName+'\\n')\n",
    "k.write(\"PEAKS\\n\")\n",
    "k.write(\"Amplitude = %d \\nMu = %0.4f \\nSigma = %0.4f\" %(Amp, Mu, Sigma)+\"\\n\")\n",
    "k.write(\"Energy resolution = %0.2f\" %(abs(energy_res))+\"%\\n\")\n",
    "k.write(\"Error in amplitude = %0.3f \\nError in mu = %0.6f \\nError in sigma = %0.6f\" %(Amp_err, Mu_err, Sigma_err)+\"\\n\")\n",
    "k.write(\"Error in energy resolution = %0.5f\"%(stdev_er)+\"%\\n\")\n",
    "k.write(\"INTEGRAL\\n\")\n",
    "k.write(\"Amplitude = %d \\nMu = %0.4f \\nSigma = %0.4f\" %(Amp_integ, Mu_integ, Sigma_integ)+\"\\n\")\n",
    "k.write(\"Energy resolution = %0.2f\" %(abs(energy_res_integ))+\"%\\n\")\n",
    "k.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you are done working with a file, ***make sure you close it!*** h5py does not like it when files are left open and you change files and/or kill the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the h5py files as text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save peaks as one-line list separated by commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=open(path+outName+\".txt\", \"a\")\n",
    "for m in peaks:\n",
    "    h.write(str(m)+', ')\n",
    "h.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save peaks as list separated by new lines (in scientific notation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=open(path+\"peaks_\"+outName+\".txt\", \"w\")\n",
    "np.savetxt(\"peaks_\"+outName+\".txt\", peaks)\n",
    "j.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOU DID IT!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
