{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to take data using the scope and plot relevant histograms. Will output energy resolution and associated errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyvisa as visa\n",
    "\n",
    "#sys.path.insert(0, '..\\scope-daq')\n",
    "\n",
    "import MSO4102Bastro as sdaq # This is the scope module that Sean G. wrote. You will need this module (should be on GitHub, https://github.com/ibrewer/scope-daq)\n",
    "import importlib\n",
    "#importlib.reload(sdaq)\n",
    "import time\n",
    "import h5py # This is the python library that creates files/stores data sets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter the IP address of the scope as a string. The IP address of the scope should be set by the router (make sure both the scope and the lab laptop are plugged into the router). To check the IP address of the scope, you can go to the Utility menu and check the LAN settings. Sometimes a LAN reset is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = sdaq.Scope(address=\"169.254.2.185\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define output file and data set names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='../dataOut/101921_amp1/'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "outName='cobalt57_14h'\n",
    "dsName='run1'\n",
    "scope.set_source_channel(0) #Set scope channel to read - same as displayed on scope\n",
    "traces=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boolean run options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePlots=True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This command creates a h5py file, desginated as \"f.\" Documentation for h5py can be found at http://docs.h5py.org/en/stable/. \n",
    "### NOTE: Please keep the 'a' flag.  ***Also, make sure you close an open file (use f.close()) before you open a new one.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = h5py.File(path+outName+'_scaling.h5py', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an array that stores the scope scaling dictionary. This will make sure that we have the scope settings for any given run.\n",
    "\n",
    "### If you want to extract the scope scaling parameters from a file, the 5 settings are stored in the order [x zero, x incr, y zero, y mult, y offset].\n",
    "### To extract the data, you could say ***data_scale = f['scope_scaling']*** and then extract the values that you want using ***data_scale[1] = x_increment***, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scaling_dict = scope.read_scaling_config()\n",
    "\n",
    "scaling_info = np.zeros(5)\n",
    "scaling_info[0] = float(scaling_dict['XZERO'])\n",
    "scaling_info[1] = float(scaling_dict['XINCR'])\n",
    "scaling_info[2] = float(scaling_dict['YZERO'])\n",
    "scaling_info[3] = float(scaling_dict['YMULT'])\n",
    "scaling_info[4] = float(scaling_dict['YOFF'])\n",
    "\n",
    "#AMANDA - debug error with same dataset name from hardcoded \"scope_scaling\"\n",
    "dset = g.create_dataset(dsName+'_scaling', data=scaling_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The get_data function will initialize a run (a full run length is 30000 traces, which takes approx. an hour and a half)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The naming scheme for data sets is filename_run#, ex. \"BC_2019703_0822_45_plateau1_initial_run1.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This function pulls SiPM pulses from the scope and stores them in arrays.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    no_of_traces: float\n",
    "        However many traces or SiPM pulses you want to record.\n",
    "    data_set_name: string\n",
    "        Name of the data set within the file. The naming scheme for data sets is \n",
    "        filename_run#, ex. \"BurstCube_PostVibe_Cs137_061419_run1.\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    Data sets. Two data sets should be created, the scope scaling information and \n",
    "    The scope data is stored in an array (a h5py data set). h5py data sets are nice \n",
    "    because you can splice into them. In this case, the size of the array is determined by the \n",
    "    number of traces you want from the scope and number of data points the scope collects for \n",
    "    each trace; the size of the array is number of traces by amount of scope points. Each row \n",
    "    is a scope trace, so plotting/data analysis is done in a for loop that looks at each row \n",
    "    one at a time.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "def get_data(run_time, data_set_name, no_of_traces = 100, noise_range = (0, 2000), signal_range = (2000,10000)):\n",
    "\n",
    "    # Determines number of points from each scope trace and creates an empty array\n",
    "    _, last_trace = scope.read_triggered_event()\n",
    "    last_trace = np.array([int(s) for s in last_trace.split(',')])\n",
    "    \n",
    "    curve_length = len(last_trace)\n",
    "    #arr = np.zeros((no_of_traces, curve_length))\n",
    "    arr = [] \n",
    "    \n",
    "    # Scaling dictionary is used to scale the scope traces to account for scope settings\n",
    "    scaling_dict = scope.read_scaling_config()\n",
    "    \n",
    "    peaks = []\n",
    "    integrals = []\n",
    "    \n",
    "    n_bad_comm = 0\n",
    "    i = 0\n",
    "    t_start = time.time()\n",
    "    t_end = t_start + run_time * 60 #run time in minutes\n",
    "    n_dup = 0\n",
    "    single = True\n",
    "    \n",
    "    time_axis = None\n",
    "    \n",
    "    bla = time.strftime('%a, %d %b %Y %H:%M:%S', time.localtime(t_end) )\n",
    "    \n",
    "    print(f\"Starting {run_time} minute run; will be done at {bla}\")\n",
    "    \n",
    "    while time.time() < t_end:\n",
    "        # Code to discount duplicates (when the scope gets stuck on a trigger):\n",
    "        \n",
    "        try: \n",
    "            _, trace = scope.read_triggered_event()\n",
    "            trace = np.array([int(s) for s in trace.split(',')])\n",
    "            if np.sum(trace - last_trace) == 0:\n",
    "                i -= 1\n",
    "                n_dup +=1\n",
    "                print(\"%d duplicates\" %(n_dup))\n",
    "            else:\n",
    "                last_trace = trace\n",
    "                time_scaled, trace_scaled = scope.scale_data(scaling_dict, trace)\n",
    "                \n",
    "                if (no_of_traces < 0) or (i < no_of_traces):\n",
    "                    arr.append(trace_scaled)\n",
    "                    time_axis = time_scaled\n",
    "                #if i % 1000 == 0:\n",
    "                #    print(\"At {0:d} / {1:d}\".format(i, no_of_traces))\n",
    "                \n",
    "                noise_sample = np.mean(trace_scaled[noise_range[0]:noise_range[1]])\n",
    "                trace_scaled -= noise_sample\n",
    "                peaks.append( np.max(trace_scaled)  )\n",
    "                integrals.append( np.sum(trace_scaled[signal_range[0]:signal_range[1]] ) )    \n",
    "                \n",
    "        \n",
    "        # Code to override Visa errors:\n",
    "        except visa.VisaIOError:\n",
    "            n_bad_comm += 1\n",
    "            print(\"Communication timeout... %d\" %(n_bad_comm))\n",
    "            i -= 1 \n",
    "        \n",
    "        i += 1       \n",
    "    \n",
    "        #if i > 0 and i %1000 == 0 and single:\n",
    "        #    t_now = time.time()\n",
    "        #    single = False \n",
    "        #    elapsed = time.time() - t_start\n",
    "        #    rate = float(i)/float(t_now - t_start)\n",
    "        #    print(\"{2:s} ; At {0:d}/{1:d}\".format(i, no_of_traces, time.strftime('%a, %d %b %Y %H:%M:%S GMT', time.localtime())))\n",
    "        #    print(\"\\tRate: {0:6.3f} Hz\\t Elapsed: {1:6.2f} s\\t Estimated total run length: {2:6.2f} s\\t Estimated time remaining: {3:6.2f}\".format(rate, elapsed, no_of_traces/(rate), no_of_traces/(rate) - elapsed))\n",
    "\n",
    "        #if i%1000 == 1:\n",
    "        #    single = True\n",
    "    \n",
    "    t_stop = time.time()\n",
    "    \n",
    "    run_len = t_stop - t_start\n",
    "    run_min = run_len / 60\n",
    "    print(f\"Recorded {i} traces in {run_min:0.3f} minutes. Average rate: {i/run_len:.2f} Hz\")\n",
    "\n",
    "    \n",
    "    dset = f.create_dataset(data_set_name, data=np.array(arr))    \n",
    "    dset = f.create_dataset(data_set_name+\"_t\", data=np.array(time_axis))   \n",
    "\n",
    "    dset = f.create_dataset(data_set_name+\"_peaks\", data=np.array(peaks))   \n",
    "    dset = f.create_dataset(data_set_name+\"_integral\", data=np.array(integrals))   \n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually take data, double check the data set name. Remember the first function input is the number of traces and the second input is the data set name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(path+outName+'.h5py', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 840 minute run; will be done at Wed, 20 Oct 2021 06:30:29\n",
      "1 duplicates\n"
     ]
    }
   ],
   "source": [
    "get_data(14*60, dsName, traces) # take data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the data sets within a file (check to make sure your run is there):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis/plotting: assign the data set as the array \"plot_array.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_array = f[dsName] # insert desired data set name here\n",
    "time_axis = np.array(f[dsName+\"_t\"])\n",
    "#sanity check\n",
    "if len(plot_array)!=traces: \n",
    "    print(\"ERROR - SOMETHING AWRY \\n dataset length is not what was input - are you looking at the right dataset?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 10% of all traces to look at the data and perform a common sense check. Also look for a stretch of data that can be used to determine noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(int(traces*0.1)):\n",
    "    plt.plot(time_axis*1e6, plot_array[i])\n",
    "    \n",
    "plt.xlabel('time [us]')\n",
    "plt.ylabel('peak amplitude [V]')\n",
    "\n",
    "if savePlots:\n",
    "    plt.savefig(path+outName+'_'+dsName+'_traces.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function will extract the peak from each trace/row of the data set. The noise sample is determined by eye; pick a range of x values where there don't appear to be (many) peaks, ex. from 2000:3500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peaks = f[dsName + '_peaks' ]\n",
    "integ = f[dsName + \"_integral\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a histogram of the peaks using matplotlib to get a sense of what the data looks like and where the peak is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ultra fine binning for debugging input\n",
    "plt.hist(peaks, bins=180)\n",
    "plt.xlabel('peak amplitude [V]')\n",
    "plt.ylabel('counts')\n",
    "\n",
    "if savePlots:\n",
    "    plt.savefig(path+outName+'_'+dsName+'_runtimePeakHist.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the same for the trace integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ultra fine binning for debugging input\n",
    "plt.hist(integ, bins=180)\n",
    "plt.xlabel('trace integral [V]')\n",
    "plt.ylabel('counts')\n",
    "\n",
    "if savePlots:\n",
    "    plt.savefig(path+outName+'_'+dsName+'_runtimeIntegHist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you are done working with a file, ***make sure you close it!*** h5py does not like it when files are left open and you change files and/or kill the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOU DID IT!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
